import re
import socket
import urllib.request
import urllib
from requests.exceptions import RequestException
import requests
import queue
import time
from bs4 import BeautifulSoup
import pymysql


def get_id(cursor):
    sql = 'select Article_name from search_article.search_result where Unit REGEXP' + "'北方工业大学'" + 'AND time=2015;'
    cursor.execute(sql)
    return cursor.fetchall()


def search_url(url):
    html = get_html(url)
    match_url = re.compile(r'<div class="ResultCont">(.*?)</div>', re.S)
    if html is None:
        return ''
    ur = match_url.findall(html)
    s = ''.join(ur)
    new_url = ''
    if ur:
        first_match = re.compile(r'</span>(.*?) target="_blank"', re.S)
        old_url = first_match.findall(s)
        if old_url:
            s2 = ''.join(old_url)
            finall_match = re.compile(r'href="(.*?)"', re.S)
            new_url = finall_match.findall(s2)
    if len(new_url) != 0:
        s3 = ''.join(new_url[0])
    else:
        s3 = ''
    return s3


def get_html(url):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.162 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=5)
        response.encoding = 'utf-8'
        if response.status_code == 200:
            return response.text
        return None
    except RequestException:
        return None
    except socket.timeout as e:
        response.close()
        return None


def crawl(url, article_name):
    html = urllib.request.urlopen(url).read().decode('utf-8')
    # 去掉注释
    re_comment = re.compile('<!--.*?-->', re.S)
    html = re_comment.sub("", html)
    soup = BeautifulSoup(html, "html.parser")
    a = soup.find_all('a', {'href': "javascript:void(0)", 'title': "知识脉络分析"})
    list1 = []
    i = 0
    for names in a:
        for s in names.stripped_strings:
            list1.insert(i, s)
    return list1


def store(cursor, direction):
    sql = 'select direction from search_article.article_direction where direction = ' + "'" + direction + "'" + ';'
    cursor.execute(sql)
    result = cursor.fetchall()
    if result:
        sql = 'select `2015_count` from search_article.article_direction where direction =' + "'" + direction + "'" + ';'
        cursor.execute(sql)
        result1 = cursor.fetchall()
        count = int(str(result1[0]).replace("(", "").replace(")", "").replace(",", "")) + 1
        sql = 'update search_article.article_direction set `2015_count`=%s where direction =' + "'" + direction + "'" + ';'
        cursor.execute(sql, count)
    else:
        sql = 'INSERT INTO search_article.article_direction (direction, 2015_count) VALUES (%s, 1)'
        cursor.execute(sql, direction)


if __name__ == '__main__':
    connect = pymysql.connect(host='localhost', port=3306, user='root', passwd='', db='Search_article', charset='utf8')
    cursor = connect.cursor()
    ids = get_id(cursor)
    articles = queue.Queue()
    for i in range(len(ids)):
        temp = str(ids[i]).replace(",", "").replace("'", "").replace("(", "").replace(")", "")
        articles.put(temp)
    while not articles.empty():
        time.sleep(0.5)
        article = articles.get()
        print(article)
        ur = 'http://www.wanfangdata.com.cn/search/searchList.do?searchType=all&showType=&pageSize=&searchWord=' + article + '&isTriggerTag='
        try:
            url = 'http://www.wanfangdata.com.cn' + search_url(ur)
        except RequestException as ex:
            print(ex)
            url = 'http://www.wanfangdata.com.cn'
        print(url)
        if url != 'http://www.wanfangdata.com.cn':
            list = crawl(url, article)
            for direction in list:
                store(cursor, direction)
